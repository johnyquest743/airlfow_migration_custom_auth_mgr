# ============================================================================
# Airflow 2.10 Complete Configuration File with OIDC Authentication
# Compatible with Python 3.12
# ============================================================================
# This is the main Airflow configuration file
# Generated for production deployment with OIDC authentication
# ============================================================================

[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# The folder where your airflow pipelines live
dags_folder = /opt/airflow/dags

# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search
remote_logging = False

# Logging level
logging_level = INFO

# Logging class
logging_config_class = 

# Log format
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

# The executor class that airflow should use
# Choices include SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
executor = LocalExecutor

# The SqlAlchemy connection string to the metadata database
# PostgreSQL (Production recommended)
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow

# MySQL alternative
# sql_alchemy_conn = mysql+mysqldb://airflow:airflow@localhost:3306/airflow

# SQLite (Development only - not for production)
# sql_alchemy_conn = sqlite:////opt/airflow/airflow.db

# The SqlAlchemy pool size is the maximum number of database connections
sql_alchemy_pool_size = 5

# The maximum overflow size of the pool
sql_alchemy_max_overflow = 10

# The SqlAlchemy pool recycle time (in seconds)
sql_alchemy_pool_recycle = 1800

# Check connection at the start of each connection pool checkout
sql_alchemy_pool_pre_ping = True

# The amount of parallelism as a setting to the executor
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
max_active_tasks_per_dag = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# When not using pools, tasks are run in the "default pool"
# whose size is guided by this config element
non_pooled_task_slot_count = 128

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Whether to load the DAG examples that ship with Airflow
load_examples = False

# Whether to load the default connections that ship with Airflow
load_default_connections = True

# Path to the folder containing Airflow plugins
plugins_folder = /opt/airflow/plugins

# Secret key to save connection passwords in the DB (Fernet key)
# Generate with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
fernet_key = YOUR_FERNET_KEY_HERE

# Whether to disable pickling dags
donot_pickle = False

# How long before timing out a python file import
dagbag_import_timeout = 30

# How long before timing out a DagFileProcessor
dag_file_processor_timeout = 50

# The class to use for running task instances in a subprocess
task_runner = StandardTaskRunner

# If set, tasks without a run_as_user argument will be run with this user
default_impersonation = 

# The amount of time (in secs) to wait before timing out on a task
default_task_execution_timeout = 

# The default task retries
default_task_retries = 0

# The default task retry delay (in seconds)
default_task_retry_delay = 300

# Should the executor assume task state remains constant between submissions of task instances
# Useful for SequentialExecutor and KubernetesExecutor
default_task_execution_timeout = 

# Dag discovery safe mode
dag_discovery_safe_mode = True

# The number of retries each task is going to have by default
default_task_retries = 0

# Updating serialized DAG can not be faster than a minimum interval
min_serialized_dag_update_interval = 30

# Fetching serialized DAG can not be faster than a minimum interval
min_serialized_dag_fetch_interval = 10

# Whether to persist DAG files code in DB
store_dag_code = True

# Maximum number of Rendered Task Instance Fields per task to store in the Database
max_num_rendered_ti_fields_per_task = 30

# On each dagrun check against defined SLAs
check_slas = True

# Path to custom XCom backend class
# xcom_backend = 

# By default Airflow plugins are lazily-loaded
lazy_load_plugins = True

# By default Airflow providers are lazily-discovered
lazy_discover_providers = True

# Hide sensitive Variables or Connection extra info
hide_sensitive_var_conn_fields = True

# A comma-separated list of extra sensitive keywords
sensitive_var_conn_names = 

# Task Slot counts for default_pool
default_pool_task_slot_count = 128

# Maximum number of tasks to run concurrently
max_active_tasks_per_dag = 16

# Minimum file parsing loop time in seconds for the scheduler
min_file_process_interval = 30

# Number of seconds after which a DAG file is parsed
dag_dir_list_interval = 300

# Whether to scan the DAGs directory for Python files
# Can be turned on when a git-sync sidecar is used
catchup_by_default = True

# The scheduler checks for heartbeats every N seconds
scheduler_health_check_threshold = 30

# ============================================================================
# AUTH MANAGER (NEW IN AIRFLOW 2.x - CRITICAL FOR OIDC)
# ============================================================================
# The auth manager class that airflow should use
# This is the most important setting for OIDC authentication
auth_manager = airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager

# ============================================================================
# [api]
# ============================================================================
[api]
# Enables the API
enable_experimental_api = False

# How to authenticate users of the API
auth_backends = airflow.api.auth.backend.session

# Maximum number of rows returned in the API response for the grid view
maximum_page_limit = 100

# ============================================================================
# [fab]
# Flask App Builder Authentication (NEW SECTION IN 2.10)
# ============================================================================
[fab]
# Comma separated list of auth backends
# This replaces the old [api] auth_backend in Airflow 2.10
auth_backends = airflow.providers.fab.auth_manager.api.auth.backend.basic_auth

# Rate limit for the API
auth_rate_limited = True
auth_rate_limit = 5 per 40 second

# ============================================================================
# [webserver]
# ============================================================================
[webserver]
# The base url of your website as airflow cannot guess what domain or
# cname you are using. This is used in automated emails that
# airflow sends to point links to the right web server
base_url = http://localhost:8080

# Default timezone to display all dates in the UI
default_ui_timezone = UTC

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Number of seconds the webserver waits before killing gunicorn master
web_server_master_timeout = 120

# Number of seconds the gunicorn webserver waits before timing out on a worker
web_server_worker_timeout = 120

# Number of workers to run the Gunicorn web server
workers = 4

# The worker class gunicorn should use
worker_class = sync

# Secret key used to run your flask app
# Generate with: openssl rand -hex 30
secret_key = YOUR_SECRET_KEY_HERE

# Expose the configuration file in the web server
expose_config = False

# Expose hostname in the web server
expose_hostname = True

# Expose stacktrace in the web server
expose_stacktrace = True

# Default DAG view
dag_default_view = grid

# Default DAG orientation
dag_orientation = LR

# The amount of time (in secs) webserver will wait for initial handshake
web_server_ssl_cert = 
web_server_ssl_key = 

# Number of seconds the gunicorn webserver waits before timing out on a worker
web_server_worker_timeout = 120

# Whether to enable log fetching from other workers
worker_refresh_batch_size = 1

# Number of seconds to wait for workers to start
worker_refresh_interval = 6000

# Secret key for Flask sessions
# This should be a random string
secret_key = YOUR_FLASK_SECRET_KEY_HERE

# Time in seconds before session expires
session_lifetime_minutes = 43200

# Enable/disable ability to view/trigger DAGs from UI for users with Viewer role
hide_paused_dags_by_default = False

# Page size in the web UI
page_size = 100

# Enable CSRF protection for web UI and API
enable_csrf = True

# Update FAB permissions and sync security manager roles on webserver startup
update_fab_perms = True

# The default UI configuration
default_ui_configuration = {'show_dag_dependencies': True}

# Maximum number of rendered task instance fields
max_ti_renders_per_page = 50

# Controls whether the web server will display the triggered DAG runs page
show_trigger_form_if_no_params = True

# How often to reload the DAG definition in seconds
reload_on_plugin_change = False

# When True, automatically run the webserver in debug mode
reload = False

# Allow the UI to be rendered in a frame
x_frame_enabled = True

# Send anonymous user activity to your analytics tool
analytics_tool = 

# Analytics ID
analytics_id = 

# Warn the user when running the webserver as root
warn_deployment_exposure = True

# Audit view permission
audit_view_excluded_events = 

# Audit view included events
audit_view_included_events = 

# The name of the header to use for user authentication
auth_backend = 

# Whether to enable auto refresh for the DAG graph
auto_refresh_interval = 3

# Enable the API
enable_swagger_ui = True

# ============================================================================
# [database]
# ============================================================================
[database]
# The SqlAlchemy connection string to the metadata database
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow

# The SqlAlchemy pool size
sql_alchemy_pool_size = 5

# The maximum overflow size of the pool
sql_alchemy_max_overflow = 10

# The SqlAlchemy pool recycle time
sql_alchemy_pool_recycle = 1800

# Check connection at the start of each connection pool checkout
sql_alchemy_pool_pre_ping = True

# The schema to use for the metadata database
sql_alchemy_schema = 

# The encoding to use for the metadata database
sql_engine_encoding = utf-8

# Collation for the metadata database
sql_engine_collation_for_ids = 

# Load default connections on database initialization
load_default_connections = True

# Maximum database query retries on operational errors
max_db_retries = 3

# ============================================================================
# [logging]
# ============================================================================
[logging]
# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search
remote_logging = False

# Logging level
logging_level = INFO

# Logging level for Flask-appbuilder UI
fab_logging_level = WARN

# Logging class
# Specify the class that will specify the logging configuration
# This class has to be on the python classpath
logging_config_class = 

# Log format for Flask-appbuilder
fab_log_format = %%(asctime)s %%(levelname)s - %%(message)s

# Log format
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s

# Simplified log format
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

# Log filename template
log_filename_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log

# Log processor filename template
log_processor_filename_template = {{ filename }}.log

# Path to custom log parser
dag_processor_manager_log_location = /opt/airflow/logs/dag_processor_manager/dag_processor_manager.log

# Name of handler to read task instance logs
task_log_reader = task

# A comma-separated list of third-party logger names that will be configured to print messages
extra_logger_names = 

# When True, logs are colored in console
colored_console_log = True

# ============================================================================
# [scheduler]
# ============================================================================
[scheduler]
# Task instances listen for external kill signal
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks
scheduler_heartbeat_sec = 5

# Statsd (https://github.com/etsy/statsd) integration settings
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

# The scheduler can run multiple threads in parallel to schedule dags
parsing_processes = 2

# Maximum number of threads to use for parsing DAGs
max_threads = 2

# Turn off scheduler catchup by setting this to False
catchup_by_default = True

# Maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# How often (in seconds) to scan the DAGs directory for new files
dag_dir_list_interval = 300

# How long before timing out a python file import while filling the DagBag
dagbag_import_timeout = 30

# Number of seconds after which a DAG file is parsed
min_file_process_interval = 30

# The number of times to try to schedule each DAG file
scheduler_health_check_threshold = 30

# Should the scheduler list new or changed DAGs after every heartbeat
list_dags_after_heartbeat = True

# Number of seconds after which a DAG run is considered orphaned
orphaned_tasks_check_interval = 300

# Maximum number of dagruns to create per DAG per minute
max_dagruns_to_create_per_loop = 10

# The scheduler will run up to this many concurrent tasks
max_tis_per_query = 512

# When using the LocalExecutor, the LocalExecutor will run up to this many concurrent tasks
local_task_job_heartbeat_sec = 5

# ============================================================================
# [celery]
# Only needed if using CeleryExecutor
# ============================================================================
[celery]
# Celery Broker URL
# For Redis: redis://localhost:6379/0
# For RabbitMQ: amqp://guest:guest@localhost:5672//
broker_url = redis://redis:6379/0

# Celery Result Backend URL
result_backend = db+postgresql://airflow:airflow@postgres:5432/airflow

# Number of messages to prefetch at a time multiplied by the number of concurrent processes
worker_prefetch_multiplier = 1

# The concurrency that will be used when starting workers
worker_concurrency = 16

# When True, tasks won't be sent to disabled workers
worker_enable_remote_control = True

# Default queue name
default_queue = default

# Celery configuration options
celery_config_options = airflow.providers.celery.executors.default_celery.DEFAULT_CELERY_CONFIG

# SSL Active
ssl_active = False

# SSL Key
ssl_key = 

# SSL Cert
ssl_cert = 

# SSL CA Cert
ssl_cacert = 

# Celery Pool implementation
pool = prefork

# Worker log server
worker_log_server_port = 8793

# Sync parallelism
sync_parallelism = 0

# ============================================================================
# [operators]
# ============================================================================
[operators]
# The default owner assigned to each new operator, unless provided explicitly or passed via default_args
default_owner = airflow

# Default CPU
default_cpus = 1

# Default RAM
default_ram = 512

# Default disk
default_disk = 512

# Default GPU
default_gpus = 0

# ============================================================================
# [smtp]
# Email configuration for alerts
# ============================================================================
[smtp]
# SMTP host
smtp_host = smtp.gmail.com

# SMTP starttls
smtp_starttls = True

# SMTP SSL
smtp_ssl = False

# SMTP user
smtp_user = your-email@example.com

# SMTP password (use secret backend in production)
smtp_password = your-password

# SMTP port
smtp_port = 587

# SMTP mail from
smtp_mail_from = airflow@example.com

# SMTP timeout
smtp_timeout = 30

# SMTP retry limit
smtp_retry_limit = 5

# ============================================================================
# [metrics]
# ============================================================================
[metrics]
# Enable sending metrics to StatsD
statsd_on = False

# StatsD host
statsd_host = localhost

# StatsD port
statsd_port = 8125

# StatsD prefix
statsd_prefix = airflow

# StatsD allow list
statsd_allow_list = 

# ============================================================================
# [secrets]
# Secrets backend for storing sensitive data
# ============================================================================
[secrets]
# Backend to use for secrets
# Options: airflow.secrets.local_filesystem.LocalFilesystemBackend
#          airflow.providers.hashicorp.secrets.vault.VaultBackend
#          airflow.providers.google.cloud.secrets.secret_manager.CloudSecretManagerBackend
backend = 

# Configuration for secrets backend
backend_kwargs = 

# ============================================================================
# [cli]
# ============================================================================
[cli]
# Colored output for CLI
api_client = airflow.api.client.local_client

# Enable color in CLI
endpoint_url = http://localhost:8080

# ============================================================================
# [debug]
# ============================================================================
[debug]
# This option will only be considered during development
# In production, Airflow will always run in non-debug mode
fail_fast = False

# ============================================================================
# [lineage]
# ============================================================================
[lineage]
# Backend to use for lineage data
backend = 

# ============================================================================
# [atlas]
# Atlas integration for lineage
# ============================================================================
[atlas]
# Atlas server endpoint
host = 

# Atlas server port
port = 21000

# Atlas username
username = 

# Atlas password
password = 

# ============================================================================
# [kubernetes]
# Only needed if using KubernetesExecutor
# ============================================================================
[kubernetes]
# Kubernetes namespace
namespace = airflow

# Path to kubeconfig file
kube_config = 

# Worker container repository
worker_container_repository = apache/airflow

# Worker container tag
worker_container_tag = 2.10.5-python3.12

# Image pull policy
worker_container_image_pull_policy = IfNotPresent

# Delete worker pods on success
delete_worker_pods = True

# Delete worker pods on failure
delete_worker_pods_on_failure = False

# Git sync container repository
git_repo = 

# Git branch
git_branch = 

# Git subpath
git_subpath = 

# ============================================================================
# [kubernetes_executor]
# ============================================================================
[kubernetes_executor]
# Multi-namespace mode
multi_namespace_mode = False

# ============================================================================
# [hive]
# ============================================================================
[hive]
# Default Hive mapred queue
default_hive_mapred_queue = 

# ============================================================================
# [elasticsearch]
# For remote logging
# ============================================================================
[elasticsearch]
# Elasticsearch host (comma separated list)
host = 

# Elasticsearch write log name
log_id_template = {dag_id}-{task_id}-{run_id}-{map_index}-{try_number}

# Elasticsearch end of log mark
end_of_log_mark = end_of_log

# Elasticsearch frontend
frontend = 

# Elasticsearch write timeout
write_stdout = False

# Elasticsearch JSON format
json_format = False

# Elasticsearch JSON fields
json_fields = asctime, filename, lineno, levelname, message

# ============================================================================
# [elasticsearch_configs]
# ============================================================================
[elasticsearch_configs]
# Maximum number of retries for Elasticsearch requests
max_retries = 3

# Elasticsearch timeout
timeout = 30

# Elasticsearch retry timeout
retry_timeout = True

# ============================================================================
# END OF CONFIGURATION FILE
# ============================================================================

# ============================================================================
# IMPORTANT NOTES:
# ============================================================================
# 1. Replace YOUR_FERNET_KEY_HERE with a key generated by:
#    python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
#
# 2. Replace YOUR_SECRET_KEY_HERE with a key generated by:
#    openssl rand -hex 30
#
# 3. Update sql_alchemy_conn with your actual database credentials
#
# 4. For OIDC authentication, the most important setting is:
#    [core] auth_manager = airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
#
# 5. Additional OIDC configuration goes in webserver_config.py (separate file)
#
# 6. For production, use PostgreSQL or MySQL, never SQLite
#
# 7. Store sensitive values in environment variables or secrets backend:
#    Example: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql://...
#
# 8. The [fab] section is new in Airflow 2.10 and replaces old [api] auth settings
#
# ============================================================================